# MÃMIR: Cognitive Diagnostic & Incident-Response Agent

An end-to-end, production-grade LLM system for DevOps, SRE, and cloud-infrastructure reasoning.

---

## ğŸ“Œ Project Overview

**MÃ­mir** is an advanced LLM-driven cognitive workflow agent designed for incident response, DevOps troubleshooting, and cloud/SRE diagnostics. It's engineered as a real-world, industry-standard AI assistant, not a toy chatbot.

It integrates several advanced AI and software engineering components:

- **RAG (Retrieval-Augmented Generation)**
- **PEFT Fine-Tuning (LoRA/QLoRA)**
- **Structured Chain-of-Thought reasoning**
- **Quantization-aware Serving (vLLM / TGI)**
- **Incident-specific Evaluation & Ablations**
- **Guardrails, safety layers, and multi-turn memory**
- **Production-quality engineering workflows**

The core goal is to analyze logs, alerts, and metrics, retrieve relevant documentation (SOPs, runbooks, docs), and reason step-by-step to produce actionable root-cause analyses and suggest safe mitigation steps consistent with SRE best practices.

---

## ğŸ’» Core Use Case: Incident Assistance

MÃ­mir assists engineers by providing structured diagnostics during incidents, including but not limited to:

- Service downtime
- Pod crash loops
- Failing deployments
- Memory/CPU saturation
- Authentication/configuration errors
- Networking issues and database latency spikes
- Container-level faults and alert rule anomalies
- CI/CD pipeline errors

---

## ğŸ§  Key Capabilities

### 1. Retrieval-Augmented Generation (RAG)

Enables grounded reasoning by fetching relevant context from a knowledge base.

- **Custom Ingestion**: Kubernetes docs, AWS/GCP playbooks, Google SRE Handbook, DevOps SOPs/runbooks, and example diagnostic cases.
- **Workflow**: Chunking, cleaning, embedding, indexing, and Top-k retrieval with context injection into the LLM.

### 2. PEFT Fine-Tuning (LoRA/QLoRA)

Uses Supervised Fine-Tuning (SFT) on structured incident reasoning and troubleshooting data.

- **Objective**: Improve reasoning structure, reduce hallucinations, and align outputs with DevOps best practices.
- **Implementation**: Open-source LLM (e.g., LLaMA/Mistral) + LoRA/QLoRA adapters, trained via TRL SFTTrainer + PEFT.

### 3. Structured Chain-of-Thought (CoT) Reasoning

The model is fine-tuned to output clean, interpretable, step-by-step reasoning using a defined schema:

1. Symptom Identification
2. Hypothesis Generation
3. Checks / Verifications
4. Root Cause Conclusion
5. Recommended Action Steps

### 4. Incident Evaluation & Ablation Harness

A full evaluation pipeline to rigorously test system performance.

- **Comparisons**: Baseline vs PEFT, RAG vs No-RAG, Zero-shot CoT vs Fine-tuned CoT.
- **Ablations**: Testing chunk size, top-k retriever variations, latency, and performance.
- **Metrics**: LLM-as-a-Judge scoring for quality, summarized in tables, plots, and a report.md.

### 5. Guardrails & Safety Layer

Ensures the agent is safe, reliable, and operates within defined boundaries.

- Rejects unsupported domains and filters junk/off-topic inputs.
- Restricts high-risk operational advice.
- Generates "safe mitigations" aligned with industry SRE practices and requires multi-step verification.

### 6. Model Serving Optimization

Engineered for high-throughput, low-latency inference.

- **Servers**: vLLM and TGI (Text Generation Inference) for robust, containerized deployment.
- **Optimization**: 4-bit quantization (QLoRA, GPTQ, AWQ) for reduced memory footprint.
- **API**: Async FastAPI inference API. Everything runs in Docker.

### 7. Multi-Turn Conversational Memory

Maintains short contextual memory to support multi-step incident resolution.

- **Mechanism**: Rolling window prompts, incident-session tracking, and token budgeting via context trimming.

---

## âš™ï¸ Technical Architecture

MÃ­mir is modularized for clarity, maintenance, and scalability.

### Core Modules

| Module | Description | Key Components |
|--------|-------------|----------------|
| `/rag_pipeline/` | Ingestion, chunking, embedding, and vector DB management. | `ingest.py`, `retriever.py` |
| `/training/` | Supervised Fine-Tuning (SFT), LoRA adapter generation, and checkpoints. | `sft_train.py`, `lora_config.yaml` |
| `/serving/` | Model deployment using vLLM/TGI and handling quantized models. | `vllm_server.py`, `tgi_server.py` |
| `/api/` | FastAPI backend, structured response schemas, and middleware. | `main.py`, `schemas.py` |
| `/eval/` | Ablations, quality metrics, and LLM-as-a-Judge scoring. | `baseline_eval.py`, `llm_judge.py` |
| `/guardrails/` | Safety classifiers, input filters, and operational safety rules. | `classifier.py`, `safety_rules.yaml` |
| `/memory/` | Multi-turn session management and conversation summarization. | `conversation_manager.py` |
| `/data/` | Storage for raw documents, processed chunks, and training/evaluation datasets. | `raw/`, `train/`, `eval/` |

### System Flow

```
User query â†’ Retriever â†’ Context selection â†’ PEFT-fine-tuned LLM â†’ CoT reasoning â†’ Final actionable output
```

---

## ğŸ“ Folder Structure

```
mimir/
â”‚
â”œâ”€â”€ rag_pipeline/           # RAG components
â”œâ”€â”€ training/               # Fine-tuning scripts & checkpoints
â”œâ”€â”€ serving/                # Model inference servers (vLLM, TGI)
â”œâ”€â”€ api/                    # FastAPI backend
â”œâ”€â”€ eval/                   # Evaluation harness and metrics
â”œâ”€â”€ guardrails/             # Safety & filtering logic
â”œâ”€â”€ memory/                 # Conversational context & session manager
â”œâ”€â”€ data/                   # All raw and processed data
â””â”€â”€ README.md
```

---

## ğŸš€ Planned Milestones

- [ ] RAG MVP working end-to-end
- [ ] CoT dataset creation and preparation
- [ ] PEFT SFT fine-tuning with LoRA completion
- [ ] vLLM inference server deployment
- [ ] Incident evaluation harness operational
- [ ] Ablation experiments and reporting
- [ ] Guardrails & safety layer integration
- [ ] Multi-turn memory implementation
- [ ] Final documentation and demo

---

## ğŸ”¥ Vision Statement

**MÃ­mir** is a serious, engineering-grade cognitive assistant for real DevOps/SRE workflows. It demonstrates the high-value skillset of modern applied AI engineering: LLM reasoning, retrieval grounding, safe automation, and production-level system design.

This project is positioned to be both research-credible and industry-admired.

---

## ğŸ“ Notes

For more details on specific components like the PEFT Fine-Tuning process or the Structured Chain-of-Thought schema, please refer to the respective module documentation or open an issue for discussion.


----------
```
mimir/
â”‚
â”œâ”€â”€ pyproject.toml               # poetry-managed project
â”œâ”€â”€ README.md                    # project overview + setup
â”œâ”€â”€ Makefile                     # convenience commands (optional)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example                 # example env vars
â”‚
â”œâ”€â”€ configs/                     # all YAML configs
â”‚   â”œâ”€â”€ training/
â”‚   â”‚    â”œâ”€â”€ sft.yaml
â”‚   â”‚    â”œâ”€â”€ lora.yaml
â”‚   â”‚    â””â”€â”€ evaluation.yaml
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚    â”œâ”€â”€ rag_config.yaml
â”‚   â”‚    â”œâ”€â”€ chunking.yaml
â”‚   â”‚    â””â”€â”€ retrieval.yaml
â”‚   â”œâ”€â”€ serving/
â”‚   â”‚    â”œâ”€â”€ vllm.yaml
â”‚   â”‚    â”œâ”€â”€ api.yaml
â”‚   â”‚    â””â”€â”€ quantization.yaml
â”‚   â””â”€â”€ project.yaml
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # raw logs, SRE docs, incidents
â”‚   â”œâ”€â”€ processed/               # cleaned + chunked docs
â”‚   â”œâ”€â”€ rag_store/               # embeddings, indices
â”‚   â”œâ”€â”€ sft/                     # fine-tuning data in JSONL
â”‚   â”‚    â”œâ”€â”€ train.jsonl
â”‚   â”‚    â”œâ”€â”€ dev.jsonl
â”‚   â”‚    â””â”€â”€ test.jsonl
â”‚   â””â”€â”€ eval/                    # eval harness datasets
â”‚
â”œâ”€â”€ mimir/                       # MAIN PYTHON PACKAGE
â”‚   â”œâ”€â”€ __init__.py
â”‚
â”‚   â”œâ”€â”€ config/                  # config loaders
â”‚   â”‚    â”œâ”€â”€ loader.py
â”‚   â”‚    â””â”€â”€ schema.py           # pydantic models
â”‚
â”‚   â”œâ”€â”€ data_pipeline/
â”‚   â”‚    â”œâ”€â”€ ingest.py           # PDF/text ingestion
â”‚   â”‚    â”œâ”€â”€ clean.py            # cleanup + normalization
â”‚   â”‚    â”œâ”€â”€ chunk.py            # chunking logic
â”‚   â”‚    â””â”€â”€ embed.py            # embedding + vector DB writer
â”‚
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚    â”œâ”€â”€ retriever.py        # vector DB retrieval (Chroma/Qdrant)
â”‚   â”‚    â”œâ”€â”€ formatter.py        # context assembly
â”‚   â”‚    â””â”€â”€ ranker.py           # optional rerankers
â”‚
â”‚   â”œâ”€â”€ sft/
â”‚   â”‚    â”œâ”€â”€ dataset_prep.py     # CoT dataset builder
â”‚   â”‚    â”œâ”€â”€ trainer.py          # TRL SFTTrainer with LoRA/QLoRA
â”‚   â”‚    â””â”€â”€ utils.py
â”‚
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚    â”œâ”€â”€ prompt_templates.py
â”‚   â”‚    â”œâ”€â”€ generator.py        # openai-like interface
â”‚   â”‚    â””â”€â”€ postprocess.py      # parse COT â†’ final answer
â”‚
â”‚   â”œâ”€â”€ serving/
â”‚   â”‚    â”œâ”€â”€ api.py              # FastAPI routes
â”‚   â”‚    â”œâ”€â”€ controllers.py      # business logic
â”‚   â”‚    â”œâ”€â”€ memory.py           # conversation memory manager
â”‚   â”‚    â””â”€â”€ guardrails.py       # safety layer
â”‚
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚    â”œâ”€â”€ evaluator.py        # eval harness (base vs RAG vs SFT)
â”‚   â”‚    â”œâ”€â”€ metrics.py          # metrics (accuracy, CoT scoring)
â”‚   â”‚    â””â”€â”€ analysis.py         # ablations + reporting
â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚    â”œâ”€â”€ logging.py          # structured logs
â”‚   â”‚    â”œâ”€â”€ exceptions.py
â”‚   â”‚    â”œâ”€â”€ text.py
â”‚   â”‚    â””â”€â”€ helpers.py
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ run_ingest.py
    â”œâ”€â”€ run_chunking.py
    â”œâ”€â”€ run_embed.py
    â”œâ”€â”€ train_sft.py
    â”œâ”€â”€ evaluate.py
    â”œâ”€â”€ build_rag.py
    â””â”€â”€ serve.py                 # launches API/vLLM/etc.
```